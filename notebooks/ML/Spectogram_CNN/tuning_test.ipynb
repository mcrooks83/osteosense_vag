{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17715657-613d-4d78-b850-c1a09049331d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_tuner as kt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "680916f4-da7d-40fb-a608-0b1a03c4a9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_to_df(path):\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "f_range = (10,950)\n",
    "\n",
    "h_data_path = f\"s_data/healthy_f_{f_range[0]}_{f_range[1]}.csv\"\n",
    "p_data_path = f\"s_data/pathology_f_{f_range[0]}_{f_range[1]}.csv\"\n",
    "\n",
    "h_df = read_csv_to_df(h_data_path)\n",
    "p_df = read_csv_to_df(p_data_path)\n",
    "\n",
    "h_data = []\n",
    "p_data = []\n",
    "\n",
    "for spectrogram_id, group in h_df.groupby('spectrogram_id'):\n",
    "    # Extract frequency, time, and power values\n",
    "    power = []\n",
    "    for f, g in group.groupby(\"frequency\"):\n",
    "        p = g['power']\n",
    "        p_num = p.to_numpy()\n",
    "        power.append(p_num)\n",
    "    p_ = np.array(power)\n",
    "    p_shaped = p_.reshape(p_.shape[0], p_.shape[1], 1)  # Shape will be (129, 41, 1)\n",
    "    h_data.append(p_shaped)\n",
    "\n",
    "for spectrogram_id, group in p_df.groupby('spectrogram_id'):\n",
    "    # Extract frequency, time, and power values\n",
    "    power = []\n",
    "    for f, g in group.groupby(\"frequency\"):\n",
    "        p = g['power']\n",
    "        p_num = p.to_numpy()\n",
    "        power.append(p_num)\n",
    "    p_ = np.array(power)\n",
    "    p_shaped = p_.reshape(p_.shape[0], p_.shape[1], 1)  # Shape will be (129, 41, 1)\n",
    "    p_data.append(p_shaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97c2aeda-4eaa-4d08-9509-e00b77300596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_class_1: (51, 257, 33, 1)\n",
      "Shape of X_class_0: (38, 257, 33, 1)\n",
      "Shape of X: (89, 257, 33, 1)\n",
      "(89,)\n"
     ]
    }
   ],
   "source": [
    "# Stack the arrays for class 1 and class 0\n",
    "X_class_1 = np.stack(h_data)  # Shape will be (n_samples_1, 129, 41, 1)\n",
    "X_class_0 = np.stack(p_data)  # Shape will be (n_samples_0, 129, 41, 1)\n",
    "\n",
    "# Print shapes to verify\n",
    "print(\"Shape of X_class_1:\", X_class_1.shape)  # Should show (n_samples_1, 129, 41, 1)\n",
    "print(\"Shape of X_class_0:\", X_class_0.shape)  # Should show (n_samples_0, 129, 41, 1)\n",
    "\n",
    "# Concatenate the stacked arrays\n",
    "X = np.concatenate((X_class_0, X_class_1), axis=0)  # Shape will be (total_samples, 129, 41, 1)\n",
    "print(\"Shape of X:\", X.shape) \n",
    "\n",
    "y_class_0 = np.zeros(X_class_0.shape[0])  # Labels for class 0\n",
    "y_class_1 = np.ones(X_class_1.shape[0])   # Labels for class 1\n",
    "y = np.concatenate((y_class_0, y_class_1))  # Combine labels\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47ebc1d5-a230-4fa8-af94-b827a6aa54a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, shuffle=True,  random_state=42)\n",
    "y_train = np.array(y_train, dtype=np.int32)  # Convert labels to int32\n",
    "y_test = np.array(y_test, dtype=np.int32)  # Convert labels to int32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8ba5a38-6dbb-4497-9866-3be6df39ad14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from my_dir/intro_to_kt/tuner0.json\n",
      "\n",
      "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
      "layer is 448 and the optimal learning rate for the optimizer\n",
      "is 0.001.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def model_builder(hp):\n",
    "  model = keras.Sequential()\n",
    " \n",
    "  model.add(Conv2D(32, (3, 3), activation='relu', ))\n",
    "\n",
    "    \n",
    "  model.add(keras.layers.Flatten())\n",
    "  # Tune the number of units in the first Dense layer\n",
    "  # Choose an optimal value between 32-512\n",
    "  hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "  model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n",
    "  model.add(keras.layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "  # Tune the learning rate for the optimizer\n",
    "  # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  return model\n",
    "\n",
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=10,\n",
    "                     factor=3,\n",
    "                     directory='my_dir',\n",
    "                     project_name='intro_to_kt')\n",
    "\n",
    "#model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "tuner.search(X_train, y_train, epochs=50, validation_split=0.2, callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "14785271-69f9-4a45-a617-04f3b806f1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"conv2d_1\" is incompatible with the layer: expected min_ndim=4, found ndim=2. Full shape received: (None, 8481)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mhypermodel\u001b[38;5;241m.\u001b[39mbuild(best_hps)\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m val_acc_per_epoch \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m best_epoch \u001b[38;5;241m=\u001b[39m val_acc_per_epoch\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mmax\u001b[39m(val_acc_per_epoch)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/layers/input_spec.py:202\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mmin_ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m ndim \u001b[38;5;241m<\u001b[39m spec\u001b[38;5;241m.\u001b[39mmin_ndim:\n\u001b[0;32m--> 202\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    203\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    204\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis incompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    205\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected min_ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mmin_ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    206\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    207\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull shape received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    208\u001b[0m         )\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# Check dtype.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"conv2d_1\" is incompatible with the layer: expected min_ndim=4, found ndim=2. Full shape received: (None, 8481)"
     ]
    }
   ],
   "source": [
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(X_train, y_train, epochs=50, validation_split=0.2)\n",
    "\n",
    "val_acc_per_epoch = history.history['val_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bcd7a90c-270b-425d-a8c7-5d8e7cb2879b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.3327 - loss: 1.6290 - val_accuracy: 0.7778 - val_loss: 3.9027\n",
      "Epoch 2/6\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8336 - loss: 1.6044 - val_accuracy: 0.7778 - val_loss: 5.6694\n",
      "Epoch 3/6\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8926 - loss: 0.1927 - val_accuracy: 0.7778 - val_loss: 10.3857\n",
      "Epoch 4/6\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8821 - loss: 0.2697 - val_accuracy: 0.7778 - val_loss: 12.3713\n",
      "Epoch 5/6\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9116 - loss: 0.2207 - val_accuracy: 0.6667 - val_loss: 13.0964\n",
      "Epoch 6/6\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9116 - loss: 0.1532 - val_accuracy: 0.6667 - val_loss: 12.8791\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x760e2eb1ef50>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypermodel = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Retrain the model\n",
    "hypermodel.fit(X_train, y_train, epochs=best_epoch, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "05ea8dcd-4efd-4f04-a989-7de83f56b4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4970 - loss: 26.0044 \n",
      "[test loss, test accuracy]: [23.605451583862305, 0.5111111402511597]\n"
     ]
    }
   ],
   "source": [
    "eval_result = hypermodel.evaluate(X_test, y_test)\n",
    "print(\"[test loss, test accuracy]:\", eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac63d2a-07ab-4ebf-9fe0-4d93fd2d5d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
